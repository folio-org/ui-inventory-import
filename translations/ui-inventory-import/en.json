{
    "meta.title": "Inventory import",

    "no-value": "-",
    "error": "XXX Something went wrong",
    "selectValue": "Select a value",
    "fillIn": "Please fill this in to continue",
    "invalidJSON": "XXX Invalid JSON: {error}",
    "invalidXML": "Invalid XML: {error}",
    "invalidXSLT": "Valid XML, but not an XSLT stylesheet",
    "validXSLT": "Valid XSLT stylesheet",
    "selectToContinue": "Please select to continue",
    "resultCount": "{count, number} {count, plural, one {record found} other {records found}}",
    "set-to-xml2json": "XXX Choose XML-to-JSON",
    "reports": "XXX Reports",
    "export-csv": "Export CSV",
    "transform": "XXX Transform",
    "settings.pipeline": "Transformation pipelines",
    "settings.pipeline.heading": "transformation pipeline",
    "settings.step": "Transformation steps",
    "settings.step.heading": "XXX Manage transformation steps",
    "settings.channels": "Channels",
    "settings.logs": "Log deletion",
    "settings.logs.heading": "XXX Manage log deletion threshold",
    "settings.logs.description": "XXX Logs older than the configured age threshold are automatically deleted once per day, at 2am Central European time. The threshhold is specified by a number and a unit. ",
    "settings.logs.number": "XXX Number",
    "settings.logs.unit": "XXX Unit",
    "settings.logs.unit.days": "XXX Days",
    "settings.logs.unit.weeks": "XXX Weeks",
    "settings.logs.unit.months": "XXX Months",

    "add": "Add",
    "actions.new": "New record",
    "actions.new.channel.xml": "XML bulk channel",

    "nav.channels": "Channels",
    "nav.jobs": "Jobs",
    "nav.jobs-for": "XXX Jobs for {name}",
    "nav.records": "Failed records",
    "nav.mike": "XXX Mike",

    "button.view-log": "XXX View log",
    "button.view-log.current": "XXX Current log",
    "button.view-log.last": "Last log",
    "button.old-jobs": "Old jobs",
    "button.start-job": "XXX Start job",
    "button.stop-job": "XXX Stop job",
    "op.delete.confirm": "XXX Are you sure you want to delete this record?",
    "op.delete.completed": "XXX Record <b>{name}</b> deleted",
    "op.run.completed": "XXX Job <b>{name}</b> started",
    "op.stop.completed": "XXX Job <b>{name}</b> stopped",
    "op.run.error": "XXX Could not start job <b>{name}</b>: {error}",
    "op.stop.error": "XXX Could not stop job <b>{name}</b>: {error}",

    "channels.column.name": "Name",
    "channels.column.lastHarvestFinished": "XXX Last harvest finished",
    "channels.column.enabled": "Enabled?",
    "channels.column.type": "Type",
    "channels.column.id": "ID",
    "channels.column.logFile": "Log file",
    "channels.column.oldJobs": "Old jobs",

    "channels.column.enabled.true": "Yes",
    "channels.column.enabled.false": "No",

    "channels.column.type.XML": "XML bulk",

    "jobs.column.status.RUNNING": "Running",
    "jobs.column.status.PAUSED": "Paused",
    "jobs.column.status.DONE": "Done",
    "jobs.column.status.INTERRUPTED": "Interrupted",

    "channels.index.all": "(All)",
    "channels.index.name": "Name",
    "channels.index.id": "ID",
    "channels.index.message": "XXX Message",

    "jobs.index.all": "(All)",
    "jobs.index.channelName": "Channel name",
    "jobs.index.message": "Message",

    "records.index.all": "(All)",
    "records.index.recordNumber": "Record number",
    "records.index.instanceHrid": "XXX Instance hrid",
    "records.index.instanceTitle": "XXX Instance title",
    "records.index.errors": "XXX Errors",
    "records.index.timeStamp": "XXX Time stamp",
    "records.index.channelName": "Channel name",

    "searchInputLabel": "XXX Search input field",
    "filter.date.started": "Date started",
    "filter.date.started.from": "From",
    "filter.date.started.to": "To",
    "filter.date.finished": "Date finished",
    "filter.date.finished.from": "From",
    "filter.date.finished.to": "To",
    "filter.date.timeStamp": "Time stamp",
    "filter.date.timeStamp.from": "From",
    "filter.date.timeStamp.to": "To",
    "filter.numeric.records": "Records",
    "filter.numeric.records.from": "At least",
    "filter.numeric.records.to": "No more than",

    "accordion.devinfo": "XXX Developer information",

    "channels.heading.general": "XXX General information",
    "channels.field.id": "Id",
    "channels.field.tag": "Tag",
    "channels.field.type": "Type",
    "channels.field.jobClass": "XXX Job class",
    "channels.field.jobClass.oaiPmh": "XXX OAI-PMH",
    "channels.field.jobClass.xmlBulk": "XXX XML bulk",
    "channels.field.jobClass.connector": "XXX Harvest connector",
    "channels.field.jobClass.status": "XXX Status report",
    "channels.field.id.help": "Automatically assigned identifier for the job",
    "channels.field.name": "Name",
    "channels.field.name.help": "Preferably a unique name for users to identify this Harvester resource. In some cases the name may be proposed after filling out protocol specific section of the configuration (e.g Index Data Connectors, OAI-PMH).",
    "channels.field.serviceProvider": "XXX Service provider",
    "channels.field.serviceProvider.help": "XXX Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "channels.field.usedBy": "XXX Used by",
    "channels.field.usedBy.help": "XXX Free-text field for tagging a job with the intended target audience, like a user group or customer of the resource. Multiple user/customer tags may be separated by commas. The tags can be used for filtering status reports by usages/customers.",
    "channels.field.managedBy": "XXX Managed by",
    "channels.field.managedBy.help": "XXX Free-text field for tagging a job with the producer or manager of the resource. Multiple tags may be separated by commas. The tags can be used for filtering status reports by job administrators.",
    "channels.field.openAccess": "XXX Open access resource?",
    "channels.field.description": "XXX Content description",
    "channels.field.description.help": "XXX Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "channels.field.technicalNotes": "XXX Technical notes",
    "channels.field.technicalNotes.help": "XXX Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "channels.field.contactNotes": "XXX Contact notes",
    "channels.field.contactNotes.help": "XXX Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "channels.field.enabled": "Channel enabled?",
    "channels.field.commissioned": "Channel commissioned?",
    "channels.field.listening": "Channel listening?",
    "channels.field.enabled.help": "Check to run the Harvesting job as described by the time/interval selected in \"Harvest schedule\". Leaving this box unchecked will make the job inactive.",
    "channels.field.scheduleString": "XXX Harvest schedule",
    "channels.field.scheduleString.help": "XXX Use these fields to define a recurring time/interval at which the Harvester job should run. E.g for weekly runs specify a day of the week on which the harvest should be executed.",
    "channels.field.scheduleString.harvest": "XXX Harvest",
    "channels.field.scheduleString.day": "XXX (day) of",
    "channels.field.scheduleString.month": "XXX (month) if it's",
    "channels.field.scheduleString.weekday": "XXX (day of the week)",
    "channels.field.scheduleString.time": "XXX Harvesting time",
    "channels.field.scheduleString.hour": "XXX (hour in 24 format)",
    "channels.field.scheduleString.minute": "XXX (min)",
    "channels.field.transformationPipeline": "Transformation pipeline",
    "channels.field.transformationPipeline.help": "Select the transformation required to match the input format delivered by the feed to the internal format used by the Harvester for data storage. See the Transformation Pipelines manual section for more details.",
    "channels.field.laxParsing": "XXX Use lax parsing (if possible)",
    "channels.field.laxParsing.help": "XXX When enabled, harvester will attempt to parse malformed XML (missing closing tags, entities)",
    "channels.field.encoding": "XXX Encoding override (ISO-8859-1, UTF-8, ...)",
    "channels.field.encoding.help": "XXX A feed can return invalid encoded responses, such as having an XML header with encoding set to UTF-8, but actually return ISO-8859-1 in the data. Setting this field to the actual encoding will force the Harvester to use the specified encoding.",
    "channels.field.cacheEnabled": "XXX Cache on disk?",
    "channels.field.cacheEnabled.help": "XXX If enabled, harvest data is kept in the filesystem cache and the job can be restarted from this cache without needing to go back to the server.",
    "channels.field.storeOriginal": "XXX Store original record content?",
    "channels.field.recordLimit": "XXX Limit record number to",
    "channels.field.recordLimit.help": "XXX Limit the harvest run to a specified number of records: useful for testing job settings and transformation pipelines.",
    "channels.field.timeout": "XXX Connection/read timeout (seconds)",
    "channels.field.timeout.help": "XXX Specify a non-default timeout value for obtaining and reading from the network connection (socket). Values under 1 minute are not recommended.",
    "channels.field.logLevel": "XXX Log level",
    "channels.field.logLevel.help": "XXX Specify the logging level for the job with DEBUG being the most verbose. INFO is the recommended log level in most cases.",
    "channels.field.failedRecordsLogging": "XXX Saving failed records",
    "channels.field.failedRecordsLogging.help": "XXX Specify whether or not failed records should be saved as XML files in a designated log directory. Also specify retention policy for the directory, that is, whether to retain files that were saved in previous runs and, if so, whether to overwrite an existing file if the same record fails again or rather add a sequence number to the new file name in order not to overwrite.",
    "channels.field.maxSavedFailedRecordsPerRun": "XXX Maximum number of failed records saved next run",
    "channels.field.maxSavedFailedRecordsPerRun.help": "XXX Sets a maximum number of files to save in the failed records directory per run. The job log will tell when the limit is reached.",
    "channels.field.maxSavedFailedRecordsTotal": "XXX Maximum number of failed records saved total",
    "channels.field.maxSavedFailedRecordsTotal.help": "XXX Sets a maximum number of files to be saved in the failed records directory at any given time - as the sum of previously saved records (that were not cleaned up before this run) plus any new records added during the run. The job log will tell when the limit is reached.",
    "channels.field.mailAddress": "XXX Notification e-mail addresses",
    "channels.field.mailAddresses": "XXX Notification e-mail addresses",
    "channels.field.mailAddresses.help": "XXX List of e-mail addresses that should receive notification on job completion.",
    "channels.field.mailLevel": "XXX Send notification if severity at least",
    "channels.field.mailLevel.help": "XXX specify job completion status with the least severity that will trigger the e-mail notification.",
    "channels.field.constantFields": "XXX List of constant fields",
    "channels.field.constantFields.help": "XXX A list of NAME=VALUE pairs. For a channel that has this field set, each harvested record has each NAME field set to the corresponding VALUE.",
    "channels.field.json": "XXX Extra configuration: (JSON)",
    "channels.field.json.help": "XXX Specify additional advanced harvester configuration in the JSON format.",

    "channels.field.type.oaiPmh": "XXX OAI-PMH specific information",
    "channels.field.url": "XXX OAI repository URL",
    "channels.field.url.help": "XXX Enter a link (http-based) to the resource to be harvested. Include the base link defined by OAI Set Name: (see below). Some resources have multiple sets within the repository. If no specific set is identified by the URL, the full repository will be harvested.",
    "channels.field.oaiSetName": "XXX OAI set name (type for suggestions)",
    "channels.field.oaiSetName.help": "XXX an optional setting, an OAI-PMH setSpec value which specifies set criteria for selective harvesting.",
    "channels.field.metadataPrefix": "XXX Metadata prefix",
    "channels.field.metadataPrefix.help": "XXX A string that specifies the metadata format in OAI-PMH requests issued to a targeted repository. It is important to choose the correct format or no data will be harvested from the repository. Make sure a Transformation Pipeline that matches the metadata format used in the repository is selected, otherwise records will not be understood by the Harvester. Repositories generally use one of the following prefixes (or embedded data formats): Dublin Core (OAI-DC) or MARC XML (MARC12/USMARC). Other less common MetadataPrefix values include PMC (PubMed Central full-text records), PMC (PubMed Central metadata records), and PZ2 (pazpar2).",
    "channels.field.dateFormat": "XXX Date format",
    "channels.field.useLongDateFormat": "XXX Use long date format",
    "channels.field.useLongDateFormat.help": "XXX Check-box to indicate whether to use a long date format when requesting records from the OAI-PMH resource. This is not used very often, but is required by some resources.",
    "channels.field.fromDate": "XXX Harvest from",
    "channels.field.fromDate.help": "XXX If empty and no resumption token is set, the Harvester will harvest the full data set from the resource. When this field contains a value, upon completion of the job the Harvester will reset the value of this field to the day prior to the current run date, so subsequent runs will harvest only new records.",
    "channels.field.untilDate": "XXX Harvest until",
    "channels.field.untilDate.help": "XXX Upper date limit for selective harvesting. On consecutive runs the Harvester will clear this field making the date interval open-ended.",
    "channels.field.resumptionToken": "XXX Resumption token (overrrides date)",
    "channels.field.resumptionToken.help": "XXX The OAI-PMH protocol supports splitting bigger datasets into smaller chunks. On delivery of a chunk of records, the OAI-PMH returns a token which the next request should use in order to get the next chunk. If an OAI-PMH job halts before completion, the resumption token will be set in this field. Sometimes it is possible to run it again from this resumption point at a later stage, but this is not always supported.",
    "channels.field.clearRtOnError": "XXX Clear resumption token on connection errors",
    "channels.field.clearRtOnError.help": "XXX Clear the resumption token for harvests that complete in an error state. This is useful when server errors out and the last resumption token is no longer valid.",
    "channels.field.keepPartial": "XXX Keep partial harvests",
    "channels.field.keepPartial.help": "XXX When checked, partial records harvested during a failed harvest run will be retained in storage.",
    "channels.field.retryCount": "XXX Request retry count",
    "channels.field.retryCount.help": "XXX Specify how many times the harvester should retry failed harvest requests, 0 disables retrying entirely.",
    "channels.field.retryWait": "XXX Delay before retry (seconds)",
    "channels.field.retryWait.help": "XXX Delay for retrying failed requests. Only change when resource fails to work with the default values.",

    "channels.field.type.xmlBulk": "XXX XML bulk specific information",
    "channels.field.urls": "XXX URLs",
    "channels.field.urls.help": "XXX One or more space-separated URL (HTTP or FTP) for XML  or MARC binary data. Jump or index pages (HTML pages with URLs) are supported and  so are FTP directories. For FTP, harvesting of recursive directories may be enabled below.",
    "channels.field.allowErrors": "XXX Continue on errors?",
    "channels.field.allowErrors.help": "XXX Check to continue harvesting and storing records even if  retrieving some of the listed resources fails.",
    "channels.field.overwrite": "XXX Overwrite data with each run (non-incremental)?",
    "channels.field.overwrite.help": "XXX Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run. This  may be used when complete catalog dumps reside on the server.    With FOLIO Inventory Storage there is no deletion of all previously harvested data, and checking  this option instead indicates that existing records should be overlaid.",
    "channels.field.allowCondReq": "XXX Ask server for new files only (incremental)?",
    "channels.field.allowCondReq.help": "XXX Ask the server if the files are  modified before attempting a harvest, relies on proper timestamp handling on the  server side. It\u2019s usually safe to have this enabled as servers are eager to  update the modification date, even in cases when the files themselves don\u2019t  change. Enabling this setting may significantly shorten harvest times.",
    "channels.field.initialFromDate": "XXX Initial from date (if incremental)",
    "channels.field.initialFromDate.help": "XXX Allows to specify the initial from  harvest date when <strong>ask server for new files only</strong> option is checked. When filled out, only files newer than the specified date will be harvested.    With FOLIO Inventory Storage, the setting additionally indicates that only incoming records  that were updated on or after this date should be loaded. Additionally, for this to take effect,  the incoming records must provide a 'lastUpdated' in the element 'processing' and on the format YYYY-MM-DD &lt;processing&gt; &lt;lastUpdated&gt;1970-01-01&lt;/lastUpdated&gt; &lt;/processing&gt; By default the logic would filter by the finishing date of the last harvest, so setting  'Initial from date' overrides the default behavior.   Following rules thus applies:   <li>If 'Overwrite data' is checked, all records are loaded.</li>  Otherwise:   <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is set,  then only records updated at or after that date will be loaded (this is regardless of  whether 'Ask server for new files only (incremental)' is checked or not)</li> <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is NOT set,  then only records updated at or after the last harvest date will be loaded</li>   But   <li>Any record without a 'lastUpdated' date will be loaded</li>",
    "channels.field.splitAt": "XXX Split XML at depth (zero/empty disables split)",
    "channels.field.splitAt.help": "XXX For XML data. This should  usually be set to 1 for XML feeds, if we want to harvest the record elements in  the data structured like:   &lt;root&gt; &nbsp;&lt;record/&gt; &nbsp;&lt;record/&gt; &lt;/root&gt;",
    "channels.field.splitSize": "XXX Split files at number of records (zero/empty disables split)",
    "channels.field.splitSize.help": "XXX The Harvester  tries to imply streaming parsing where possible, but many XSL Transformations  will not support this. Attempting to transform millions of records will be too  memory consuming, so breaking the resource into chunks of 1000 records seems to  be a reasonable option. Enter into this field the number of records to be  contained in each chunk.",
    "channels.field.expectedSchema": "XXX Mime-type override (e.g: application/marc; charset=MARC-8)",
    "channels.field.expectedSchema.help": "XXX The Harvester detects  the type (XML vs MARC binary) from the MIME-type and file extension. It is also able  to deal with compressed archives (zip, tar, gzip), in some rare case it may be  required to provide the content type manually (e.g if it\u2019s missing or wrong),  the format is:   MIME-type [; optional character encoding].",
    "channels.field.outputSchema": "XXX MARC XML transformation format (application/marc or application/tmarc)",
    "channels.field.outputSchema.help": "XXX This field  expresses the output format of binary MARC reading\u2013which will also be the input  format for the transformation pipeline. If the Transformation Pipeline expects MARC21  XML, this should be set to Application/marc. If the pipeline expects Turbo MARC XML,  it should be set to Application/tmarc.",
    "channels.field.recurse": "XXX Recurse into subfolders?",
    "channels.field.recurse.help": "XXX When set, the harvester will traverse the entire directory  tree and search for harvestable files. This setting should be enabled with care.",
    "channels.field.includeFilePattern": "XXX Include files (regular expression)",
    "channels.field.includeFilePattern.help": "XXX This setting can be used to filter what files  to harvest.  The filter applies to files in FTP directories as well as in archives (ZIP/tar).  When set to a regular expression, the harvester  will only harvest files with names matching the regular expression (unless the file name is  at the same time excluded by an exclude pattern).   Example:  <pre>.*\\.xml|.*\\.marc</pre>  Would include only .xml and .marc files.    Note that file name dots must be escaped.    Note that ZIP and tar files (.zip,.gz,.tar) are loaded even if they are not specified in the  include pattern. To enforce exclusion of ZIP or tar files they would have to be specified in  an exclude pattern (see help text for that).",
    "channels.field.excludeFilePattern": "XXX Exclude files (regular expression)",
    "channels.field.excludeFilePattern.help": "XXX This setting can be used to filter what files to harvest. The filter applies to files in FTP directories as well as entries in archives (ZIP/tar).  When set to a regular expression, the harvester  will skip any file with a file name matching the expression. Example: <pre>readme\\.txt|README|.*\\.jpg|.*\\.gif</pre>  Would exclude files with names readme.txt, README as well as .jpg and .gif files  from FTP directories or ZIP/tar archives.",
    "channels.field.passiveMode": "XXX Use passive mode for FTP transfers?",
    "channels.field.passiveMode.help": "XXX When set passive, instead of active, mode is  used for FTP connections. If harvester is running within a restricted firewall that  blocks FTP active mode connections, enabling this setting might help. It might be,  however, necessary to align this mode with what FTP server expects.",
    "channels.field.csvConfiguration": "XXX CSV parser configuration",
    "channels.field.csvConfiguration.help": "XXX The harvester will detect (either by MIME-type or by file extension)  and attempt to parse CSV (comma separated values) files into an XML representation for further processing. The XML representation of each  data row looks as follows: <pre>&lt;row&gt;&lt;field name=&quot;column name or number&quot;&gt;field value&lt;/field&gt;...&lt;/row&gt;</pre>  Unless the split at depth option is set to > 0, all rows will be parsed into a single XML document and wrapped with an additional &lt;rows&gt; root element. For large CSV files it may be a good idea to set the split at depth to 1.  The parser configuration is expressed in a semicolon delimited key/value list, like so: key1=value1; key2=value2. List of supported options is as follows: <ul> <li> charset: default \"iso-8859-1\", specifies the character encoding of the files</li> <li> delimiter: default \",\" for CSV and \"\\t\" for TSV, specifies the field delimiter used in the files</li> <li> containsHeader: default \"yes\", specifies if the first line in the files contains the header line</li> <li> headerLine: no default, allows to override or specify headers, format is a comma-separated list e.g headers=\"title,author,description\"</li></ul>",

    "channels.field.type.connector": "XXX Connector specific information",
    "channels.field.connectorEngineUrlSetting.label": "XXX CF Engine",
    "channels.field.connectorEngineUrlSetting.label.help": "XXX Select the Connector Engine instance that will be used to execute  the Connector harvesting job. The default engine is hosted by Index Data but may be  also installed locally on the customer site. Additional Connector Engines can be  specified through the Settings tab.",
    "channels.field.engineParameters": "XXX Engine parameters (optional)",
    "channels.field.engineParameters.help": "XXX Additional or custom values of Connector Engine  session parameters used by this job. See CFWS manpage for more information.",
    "channels.field.connectorRepoUrlSetting.label": "XXX CF Repository",
    "channels.field.connectorRepoUrlSetting.label.help": "XXX Select the connector repository where the Connectors are hosted  and maintained. Usually, the Connector Repository is provided by Index Data and may  require a login account. The account credentials are provided directly in the  Connector Repository URL setting accessed from the Settings tab and should have the  form: <pre>http(s)://&lt;repouser&gt;:&lt;repopass&gt;@url.to.the.repository</pre>.",
    "channels.field.connector": "XXX Connector (type for suggestion)",
    "channels.field.connector.help": "XXX Enter here the name of the harvesting  connector specific to the harvested resource. This field provides suggestions by  looking up the Repository so only a couple of initial characters or a part of the  name is required.",
    "channels.field.overwrite-connector.help": "XXX Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run.",  
    "channels.field.connuser": "XXX User Name",
    "channels.field.connuser.help": "XXX User name required for access to a harvested resource that  requires authentication.",
    "channels.field.password": "XXX Password",
    "channels.field.password.help": "XXX Password required for access to a harvested resource that requires  authentication.",
    "channels.field.proxy": "XXX Proxy server address",
    "channels.field.proxy.help": "XXX Address of the proxy server that should be used by the  harvesting engine, e.g to deal with cases when the resource is IP authenticated.",
    "channels.field.initData": "XXX Init Data",
    "channels.field.initData.help": "XXX Advanced setting to provide additional initialization parameters  to the harvesting connector. Any username/password/proxy specified in the inputs  above will take precedence over settings specified in this field. These settings  must be provided in JSON format.",
    "channels.field.startToken": "XXX Start token (incremental harvest)",
    "channels.field.startToken.help": "XXX The use of a start token for incremental  harvesting is connector specific and depends on the connector capability. This setting  must be provided in JSON format.",
    "channels.field.sleep": "XXX Delay between requests (milliseconds)",
    "channels.field.sleep.help": "XXX Delay between requests made from the  harvester to the connector engine. Use when the resource is sensitive to high loads.",
    "channels.field.failedRetryCount": "XXX Failed request retry count",
    "channels.field.failedRetryCount.help": "XXX Specify how many times the harvester should retry  failed harvest requests, 0 disables retrying entirely.",

    "channels.field.type.status": "XXX General information",
    "channels.field.usageTags": "XXX Filter: list of usage tags",
    "channels.field.usageTags.help": "XXX Used for filtering the status report by the user groups on customers tagged to a harvest job (\"Used by\").",
    "channels.field.adminTags": "XXX Filter: list of admin tags",
    "channels.field.adminTags.help": "XXX Used for filtering the status report by the harvestables creator or administrator, as tagged to the harvest job (\"Managed by\")",
    "channels.field.statusJobEnabled": "XXX Status job enabled",
    "channels.field.statusJobEnabled.help": "XXX ",
    "channels.field.customMailAddresses": "XXX Custom e-mails addresses (multiple separated with comma)",
    "channels.field.customMailAddresses.help": "XXX ",

    "channels.heading.status": "XXX Status information",
    "channels.field.initiallyHarvested": "XXX Initial harvest",
    "channels.field.lastHarvestStarted": "XXX Last harvest started",
    "channels.field.lastUpdated": "XXX Last updated",
    "channels.field.message": "XXX Message from last harvest:",

    "jobs.caption.notComplete": "Job is not complete",
    "jobs.caption.completed": "Imported {count} records in {seconds} seconds",
    "jobs.field.transformationPipeline": "Transformation pipeline",
    "jobs.field.channelName": "Channel name",  
    "jobs.field.failedRecordsLogging.NO_STORE": "XXX Don't save failed records",
    "jobs.field.failedRecordsLogging.CLEAN_DIRECTORY": "XXX Do save. Clean up directory first",
    "jobs.field.failedRecordsLogging.CREATE_OVERWRITE": "XXX Do save. Overwrite existing files",
    "jobs.field.failedRecordsLogging.ADD_ALL": "XXX Do save. Add numbered versions for existing files",
  
    "records.field.recordNumber": "Record number",
    "records.field.instanceHrid": "Instance HRID",
    "records.field.instanceTitle": "Instance title",
    "records.field.errors": "Errors",
    "records.field.timeStamp": "Time stamp",
    "records.field.channelName": "Channel name",
    "records.field.importJob": "Import job",
    "records.field.sourceFileName": "Source file name",
    "records.field.originalRecord": "Original record",
    "records.field.transformedRecord": "Transformed record",

    "logs.field.timeStamp": "Timestamp",
    "logs.field.line": "Line",

    "pipeline.field.name": "Name",
    "pipeline.field.description": "Description",
    "pipeline.field.steps": "Transformation steps in pipeline",
    "pipeline.steps.position": "#",
    "pipeline.steps.name": "Name",
    "pipeline.steps.actions": "XXX Actions",
    "step.field.name": "Name",
    "step.field.description": "Description",
    "step.field.type": "Type",
    "step.field.script": "Script",

    "logs.plainTextLog.running": "XXX Live plain text log for current running job",
    "logs.plainTextLog.previous": "XXX Plain text log for last job",
    "logs.plainTextLog.refresh": "XXX Refresh",
    "logs.plainTextLog.download": "XXX Download",
    "logs.countFailedRecords": "{count, number} {count, plural, one {failed record} other {failed records}}",
    "logs.countLogLiines": "{count, number} {count, plural, one {log line} other {log lines}}",

    "summary-table.summary": "XXX Summary",
    "summary-table.instances": "XXX Instance",
    "summary-table.holdings": "XXX Holdings",
    "summary-table.items": "XXX Item",

    "summary-label.processed": "XXX Processed",
    "summary-label.loaded": "XXX Loaded",
    "summary-label.deleted": "XXX Deleted(skipped)",
    "summary-label.failed": "XXX Failed",

    "jobs.column.channelName": "Channel name",
    "jobs.column.status": "Status",
    "jobs.column.amountImported": "Records",
    "jobs.column.seconds": "Seconds",
    "jobs.column.started": "Started",
    "jobs.column.finished": "Finished",
    "jobs.column.type": "Type",
    "jobs.column.message": "Message (processed/loaded/deleted(skipped)/failed)",

    "stats.instances": "XXX Instances",
    "stats.holdings": "XXX Holdings",
    "stats.items": "XXX Items",

    "error.invalidSort.label": "XXX Invalid sort criterion",
    "error.invalidSort.content": "XXX It is not possible to sort on the <code>{name}</code> column due to implementation limitations.",

    "permission.settings.view": "XXX Inventory import (Settings): View transformation pipelines and transformation steps, and log-file deletion threshold",
    "permission.settings.view-edit": "XXX Inventory import (Settings): View, edit transformation pipelines and transformation steps, and log-file deletion threshold",
    "permission.settings.view-edit-create": "XXX Inventory import (Settings): View, edit, create transformation pipelines and transformation steps",
    "permission.settings.view-edit-create-delete": "XXX Inventory import (Settings): View, edit, create, delete transformation pipelines and transformation steps",
    "permission.channels.view": "XXX Inventory import: View channels",
    "permission.channels.view-edit": "XXX Inventory import: View, edit channels, start/stop jobs",
    "permission.channels.view-edit-create": "XXX Inventory import: View, edit, create channels, start/stop jobs",
    "permission.channels.view-edit-create-delete": "XXX Inventory import: View, edit, create, delete channels, start/stop jobs",
    "permission.jobs-and-failed-records.view": "XXX Inventory import: View jobs and failed records",
    "permission.all": "XXX Inventory import: All permissions",

    "SENTINEL": "XXX SENTINEL"
}
